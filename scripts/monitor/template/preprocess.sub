#!/bin/bash
#SBATCH --nodes=1                # node count
#SBATCH --ntasks=1               # total number of tasks across all nodes
#SBATCH --cpus-per-task=1        # CPUS to use when using data parallelization
#SBATCH --time=05:00:00          # total run time limit (HH:MM:SS)

# A slurm script to run ALL of preprocessing

# load functions and variables
source /home/data/NDClab/tools/lab-devOps/scripts/monitor/tools.sh

# load modules
module load singularity-3.5.3
module load r-4.0.2-gcc-8.2.0-tf4pnwr
module load miniconda3-4.5.11-gcc-8.2.0-oqs2mbg

# automatically get project name
output_path="${dataset}/derivatives/preprocessed"
data_source="${dataset}/sourcedata/checked/redcap/"

# constant paths
sing_image="/home/data/NDClab/tools/instruments/containers/singularity/inst-container.simg"
json_scorer="/home/data/NDClab/tools/instruments/scripts/json_scorer.py"
survey_data="/home/data/NDClab/tools/instruments/scripts/surveys.json"

# get most recent redcap file for processing

# run instruments to preprocess survey data
singularity exec --bind $dataset,/home/data/NDClab/tools/instruments \
    $sing_image \
    python3 $json_scorer \
    $data_source/$input_file \
    $survey_data \
    $output_path

# update central tracker using instruments data
# todo: need to programtically get output file
:'
python inst-tracker.py "${output_path}/202201v0readAloudval_SCRD_2022-06-04_1550.csv"

# run R scripts that require R modules
singularity exec --bind $dataset /home/data/NDClab/tools/containers/R-4.1.2/R-con.simg Rscript "${dataset}" ; \
                                                                                       Rscript "${dataset}" ; \
										                                               Rscript "${dataset}"
'